{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for Lemmatization, remove stopword and feature selection using POS, spacy package\n",
    "import spacy\n",
    "def spacy_preprocess(text,lemma= True, pos= True, pos_select = [\"VERB\", \"NOUN\", \"ADJ\",\"ADV\",\"PART\"]):\n",
    "  # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner']) # disable parser, ner for faster loading\n",
    "  # Parse the sentence using the loaded 'en' model object `nlp`\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    if pos== False:\n",
    "        if lemma== True: text_preprocess= \" \".join([token.lemma_.lower() for token in doc if not nlp.vocab[token.text].is_stop])\n",
    "        if lemma== False:text_preprocess= \" \".join([token.text.lower() for token in doc if not nlp.vocab[token.text].is_stop])\n",
    "    else:\n",
    "        if lemma== True : text_preprocess= \" \".join([token.lemma_.lower() for token in doc if (token.pos_ in pos_select and not nlp.vocab[token.text].is_stop)])\n",
    "        if lemma== False : text_preprocess= \" \".join([token.text.lower() for token in doc if (token.pos_ in pos_select  and not nlp.vocab[token.text].is_stop)])\n",
    "  # nlp.vocab[token.text].is_stop to remove stopwords\n",
    "    return text_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing data with spacy\n",
    "news_preprocess=[]\n",
    "for filename, text in news.items():\n",
    "    preprocess = spacy_preprocess(text,pos_select = [\"VERB\", \"NOUN\", \"ADJ\"])\n",
    "    news_preprocess.append(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be modify \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=0.1, max_df=0.8, max_features= None)\n",
    "feature_matrix = vectorizer.fit_transform(news_preprocess).astype(float)\n",
    "feature_names = vectorizer.get_feature_names() # get feature names\n",
    "print(\"number of feature:\", len(feature_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
