{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "# instantiating and generating the count matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "path_df = \"Pickles/News_dataset.pickle\"\n",
    "\n",
    "with open(path_df, 'rb') as data:\n",
    "    df = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bag_of_words'] = ''\n",
    "columns = df.columns\n",
    "for index, row in df.iterrows():\n",
    "    words = []\n",
    "    Words = ''\n",
    "    for col in columns:\n",
    "        if col == 'Content':\n",
    "            #print(row[col])\n",
    "            words += row[col].split()\n",
    "    words = list(set(words))\n",
    "    for w in words:\n",
    "        Words += str(w) + \" \"\n",
    "    row['bag_of_words'] = Words\n",
    "    \n",
    "#df.drop(columns = [col for col in df.columns if col!= 'bag_of_words'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.13302661, 0.87142125, ..., 0.1405915 , 0.13693064,\n",
       "        0.16244657],\n",
       "       [0.13302661, 1.        , 0.13558154, ..., 0.19844242, 0.16777359,\n",
       "        0.19035706],\n",
       "       [0.87142125, 0.13558154, 1.        , ..., 0.14701731, 0.16572815,\n",
       "        0.17384487],\n",
       "       ...,\n",
       "       [0.1405915 , 0.19844242, 0.14701731, ..., 1.        , 0.1694113 ,\n",
       "        0.27694817],\n",
       "       [0.13693064, 0.16777359, 0.16572815, ..., 0.1694113 , 1.        ,\n",
       "        0.33951236],\n",
       "       [0.16244657, 0.19035706, 0.17384487, ..., 0.27694817, 0.33951236,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer()\n",
    "count_matrix = count.fit_transform(df['bag_of_words'])\n",
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>bag_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to continue please click the box below to let ...</td>\n",
       "      <td>central</td>\n",
       "      <td>China Downplays Chances for Trade Talks While ...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2019-0...</td>\n",
       "      <td>service cookie support Downplays javascript th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>save stories to read later or create you own ...</td>\n",
       "      <td>central</td>\n",
       "      <td>Reactions to the Coronavirus_w.allsides.c</td>\n",
       "      <td>https://www.allsides.com/news/2020-01-26-1735/...</td>\n",
       "      <td>sound parent suicide center b many andy it bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to continue please click the box below to let ...</td>\n",
       "      <td>central</td>\n",
       "      <td>Trump to Meet China's Liu in a Sign Trade Talk...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2019-0...</td>\n",
       "      <td>service cookie support javascript this inquiri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>keep me logged in world health officials said ...</td>\n",
       "      <td>central</td>\n",
       "      <td>WHO officials say coronavirus outbreak in Iran...</td>\n",
       "      <td>https://www.cnbc.com/2020/02/21/who-officials-...</td>\n",
       "      <td>likely reserved weaker with business remdesivi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to continue please click the box below to let ...</td>\n",
       "      <td>central</td>\n",
       "      <td>China Heads Into Trade Talks Bracing for More ...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2018-1...</td>\n",
       "      <td>service cookie support javascript this Heads B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content Category  \\\n",
       "0  to continue please click the box below to let ...  central   \n",
       "1   save stories to read later or create you own ...  central   \n",
       "2  to continue please click the box below to let ...  central   \n",
       "3  keep me logged in world health officials said ...  central   \n",
       "4  to continue please click the box below to let ...  central   \n",
       "\n",
       "                                               Title  \\\n",
       "0  China Downplays Chances for Trade Talks While ...   \n",
       "1          Reactions to the Coronavirus_w.allsides.c   \n",
       "2  Trump to Meet China's Liu in a Sign Trade Talk...   \n",
       "3  WHO officials say coronavirus outbreak in Iran...   \n",
       "4  China Heads Into Trade Talks Bracing for More ...   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.bloomberg.com/news/articles/2019-0...   \n",
       "1  https://www.allsides.com/news/2020-01-26-1735/...   \n",
       "2  https://www.bloomberg.com/news/articles/2019-0...   \n",
       "3  https://www.cnbc.com/2020/02/21/who-officials-...   \n",
       "4  https://www.bloomberg.com/news/articles/2018-1...   \n",
       "\n",
       "                                        bag_of_words  \n",
       "0  service cookie support Downplays javascript th...  \n",
       "1  sound parent suicide center b many andy it bre...  \n",
       "2  service cookie support javascript this inquiri...  \n",
       "3  likely reserved weaker with business remdesivi...  \n",
       "4  service cookie support javascript this Heads B...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes in movie title as input and returns the top 10 recommended movies\n",
    "def recommendations(title, cosine_sim = cosine_sim):\n",
    "    \n",
    "    recommended_news = []\n",
    "    # gettin the index of the movie that matches the title\n",
    "    #idx = indices[].index[0]\n",
    "    #idx = df.loc[df.Title == title].index[0]\n",
    "    indices = pd.Series(df.index)\n",
    "    idx = indices[indices == title].index[0]\n",
    "    print(idx)\n",
    "    # creating a Series with the similarity scores in descending order\n",
    "    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)\n",
    "\n",
    "    # getting the indexes of the 10 most similar movies\n",
    "    top_10_indexes = list(score_series.iloc[1:11].index)\n",
    "    \n",
    "    # populating the list with the titles of the best 10 matching movies\n",
    "    for i in top_10_indexes:\n",
    "        recommended_news.append(list(df.index)[i])\n",
    "        \n",
    "    return recommended_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-bc972e01dcff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'China Trade Deal_w.theflipsid'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecommendations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-32649e73b4e0>\u001b[0m in \u001b[0;36mrecommendations\u001b[1;34m(title, cosine_sim)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#idx = df.loc[df.Title == title].index[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# creating a Series with the similarity scores in descending order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3957\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3958\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3960\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "title = 'China Trade Deal'\n",
    "rec = recommendations(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Mode: tf-idf with cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to continue please click the box below to let ...</td>\n",
       "      <td>central</td>\n",
       "      <td>China Downplays Chances for Trade Talks While ...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2019-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>save stories to read later or create you own ...</td>\n",
       "      <td>central</td>\n",
       "      <td>Reactions to the Coronavirus</td>\n",
       "      <td>https://www.allsides.com/news/2020-01-26-1735/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to continue please click the box below to let ...</td>\n",
       "      <td>central</td>\n",
       "      <td>Trump to Meet China's Liu in a Sign Trade Talk...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2019-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>keep me logged in world health officials said ...</td>\n",
       "      <td>central</td>\n",
       "      <td>WHO officials say coronavirus outbreak in Iran...</td>\n",
       "      <td>https://www.cnbc.com/2020/02/21/who-officials-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to continue please click the box below to let ...</td>\n",
       "      <td>central</td>\n",
       "      <td>China Heads Into Trade Talks Bracing for More ...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2018-1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content Category  \\\n",
       "0  to continue please click the box below to let ...  central   \n",
       "1   save stories to read later or create you own ...  central   \n",
       "2  to continue please click the box below to let ...  central   \n",
       "3  keep me logged in world health officials said ...  central   \n",
       "4  to continue please click the box below to let ...  central   \n",
       "\n",
       "                                               Title  \\\n",
       "0  China Downplays Chances for Trade Talks While ...   \n",
       "1                       Reactions to the Coronavirus   \n",
       "2  Trump to Meet China's Liu in a Sign Trade Talk...   \n",
       "3  WHO officials say coronavirus outbreak in Iran...   \n",
       "4  China Heads Into Trade Talks Bracing for More ...   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.bloomberg.com/news/articles/2019-0...  \n",
       "1  https://www.allsides.com/news/2020-01-26-1735/...  \n",
       "2  https://www.bloomberg.com/news/articles/2019-0...  \n",
       "3  https://www.cnbc.com/2020/02/21/who-officials-...  \n",
       "4  https://www.bloomberg.com/news/articles/2018-1...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "path_df = \"Pickles/News_dataset.pickle\"\n",
    "\n",
    "with open(path_df, 'rb') as data:\n",
    "    df = pickle.load(data)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bag_of_words'] = ''\n",
    "columns = df.columns\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    words = []\n",
    "    Words = ''\n",
    "    for col in columns:\n",
    "        if col == 'Content'or col == 'Title':\n",
    "            #print(row[col])\n",
    "            words += row[col].split()\n",
    "            \n",
    "    words = list(set(words))\n",
    "    \n",
    "    row['bag_of_words'] = words\n",
    "# Processing Keywords\n",
    "\n",
    "tmp = df['bag_of_words'].tolist()\n",
    "key_words = []\n",
    "for l in tmp:\n",
    "    for words in l:\n",
    "        key_words.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>bag_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China Downplays Chances for Trade Talks While ...</td>\n",
       "      <td>[inquiries, cookie, let, to, Downplays, cookie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reactions to the Coronavirus</td>\n",
       "      <td>[detailed, we, known, traditionally, even, wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump to Meet China's Liu in a Sign Trade Talk...</td>\n",
       "      <td>[inquiries, cookie, let, to, cookies, are, Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WHO officials say coronavirus outbreak in Iran...</td>\n",
       "      <td>[concerning, we, business, gilead, on, worriso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China Heads Into Trade Talks Bracing for More ...</td>\n",
       "      <td>[inquiries, cookie, let, to, More, cookies, ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  China Downplays Chances for Trade Talks While ...   \n",
       "1                       Reactions to the Coronavirus   \n",
       "2  Trump to Meet China's Liu in a Sign Trade Talk...   \n",
       "3  WHO officials say coronavirus outbreak in Iran...   \n",
       "4  China Heads Into Trade Talks Bracing for More ...   \n",
       "\n",
       "                                        bag_of_words  \n",
       "0  [inquiries, cookie, let, to, Downplays, cookie...  \n",
       "1  [detailed, we, known, traditionally, even, wor...  \n",
       "2  [inquiries, cookie, let, to, cookies, are, Tra...  \n",
       "3  [concerning, we, business, gilead, on, worriso...  \n",
       "4  [inquiries, cookie, let, to, More, cookies, ar...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df = df[['Title', 'bag_of_words']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#!pip install -U gensim\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(117235 unique tokens: ['Chances', 'China', 'Downplays', 'Plays', 'Talks']...)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3aee4c495419>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarities\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatrixSimilarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Create the similarity data structure. This is the most important part where we get the similarities between the movies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0msims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatrixSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\docsim.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_best, dtype, num_features, chunksize, corpus_len)\u001b[0m\n\u001b[0;32m    831\u001b[0m                     \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse2full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocno\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py\u001b[0m in \u001b[0;36msparse2full\u001b[1;34m(doc, length)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \"\"\"\n\u001b[1;32m--> 397\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# fill with zeroes (default value)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m     \u001b[1;31m# convert indices to int as numpy 1.12 no longer indexes by floats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processed_keywords = df['bag_of_words'].to_list()\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "dictionary = Dictionary(processed_keywords) # create a dictionary of words from our keywords\n",
    "print(dictionary)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_keywords] \n",
    "#create corpus where the corpus is a bag of words for each document\n",
    "\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus) #create tfidf model of the corpus\n",
    "\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "# Create the similarity data structure. This is the most important part where we get the similarities between the movies.\n",
    "sims = MatrixSimilarity(tfidf[corpus], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_recommendation(title, number_of_hits=5):\n",
    "    dff = df.loc[df.Title==title] # get the movie row\n",
    "    keywords = dff['bag_of_words'].iloc[0] #get the keywords as a Series (movie['keywords']),\n",
    "    # get just the keywords string ([0]), and then convert to a list of keywords (.split(',') )\n",
    "    query_doc = keywords #set the query_doc to the list of keywords\n",
    "    \n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) # get a bag of words from the query_doc\n",
    "    query_doc_tfidf = tfidf[query_doc_bow] #convert the regular bag of words model to a tf-idf model where we have tuples\n",
    "    # of the movie ID and it's tf-idf value for the movie\n",
    "\n",
    "    similarity_array = sims[query_doc_tfidf] # get the array of similarity values between our movie and every other movie. \n",
    "    #So the length is the number of movies we have. To do this, we pass our list of tf-idf tuples to sims.\n",
    "\n",
    "    similarity_series = pd.Series(similarity_array.tolist(), index=df.Title.values) #Convert to a Series\n",
    "    top_hits = similarity_series.sort_values(ascending=False)[1:number_of_hits+1] \n",
    "    #get the top matching results, i.e. most similar movies; start from index 1 because every movie is most similar to itself\n",
    "\n",
    "    #print the words with the highest tf-idf values for the provided movie:\n",
    "    sorted_tfidf_weights = sorted(tfidf[corpus[df.index.values.tolist()[0]]], key=lambda w: w[1], reverse=True)\n",
    "    print('The top 5 words associated with this movie by tf-idf are: ')\n",
    "    for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "        print(\" '%s' with a tf-idf score of %.3f\" %(dictionary.get(term_id), weight))\n",
    "    \n",
    "    # Print the top matching movies\n",
    "    print(\"Our top %s most similar movies for movie %s are:\" %(number_of_hits, title))\n",
    "    for idx, (movie,score) in enumerate(zip(top_hits.index, top_hits)):\n",
    "        print(\"%d %s with a similarity score of %.3f\" %(idx+1, movie, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 words associated with this movie by tf-idf are: \n",
      " 'Plays' with a tf-idf score of 0.344\n",
      " 'Tricks’_w.bloomberg.' with a tf-idf score of 0.344\n",
      " '‘Little' with a tf-idf score of 0.344\n",
      " 'Downplays' with a tf-idf score of 0.303\n",
      " 'Chances' with a tf-idf score of 0.262\n",
      "Our top 10 most similar movies for movie China Trade Deal_w.theflipsid are:\n",
      "1 How strong is Trump's hand in US-China trade war?_w.aljazeera. with a similarity score of 0.156\n",
      "2 How Trump Can Make Trade with China Work for America_.nationalrev with a similarity score of 0.139\n",
      "3 Trump hits China with $50B tariffs, investment restrictions_w.politico.c with a similarity score of 0.137\n",
      "4 White House Tries to Tamp Down Trade War Fears as China Retaliates_w.nytimes.co with a similarity score of 0.131\n",
      "5 Trump Gets His Trade Deal, China Gets the Win_w.nytimes.co with a similarity score of 0.131\n",
      "6 Chinese leaders ‘absolutely confused’ by Trump’s demands on trade_w.politico.c with a similarity score of 0.130\n",
      "7 China Tariffs_w.theflipsid with a similarity score of 0.126\n",
      "8 U.S. to Put Tariffs on Chinese Goods, Drawing Vow of Retaliation as Trade Fight Widens_w.nytimes.co with a similarity score of 0.124\n",
      "9 Trade war threatens to roil 2020 race as Republicans complain about the tariffs Trump loves_w.washington with a similarity score of 0.122\n",
      "10 The ‘giant hole’ in Trump’s new China deal_w.politico.c with a similarity score of 0.119\n"
     ]
    }
   ],
   "source": [
    "movie_recommendation(title, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_recommendation(keywords, number_of_hits):\n",
    "    query_doc_bow = dictionary.doc2bow(keywords) # get a bag of words from the query_doc\n",
    "    query_doc_tfidf = tfidf[query_doc_bow] #convert the regular bag of words model to a tf-idf model where we have tuples\n",
    "    # of the movie ID and it's tf-idf value for the movie\n",
    "\n",
    "    similarity_array = sims[query_doc_tfidf] # get the array of similarity values between our movie and every other movie. \n",
    "    #So the length is the number of movies we have. To do this, we pass our list of tf-idf tuples to sims.\n",
    "\n",
    "    similarity_series = pd.Series(similarity_array.tolist(), index=df.Title.values) #Convert to a Series\n",
    "    top_hits = similarity_series.sort_values(ascending=False)[:number_of_hits] #get the top matching results, \n",
    "    # i.e. most similar movies\n",
    "\n",
    "    # Print the top matching movies\n",
    "    print(\"Our top %s most similar movies for the keywords %s are:\" %(number_of_hits, keywords))\n",
    "    for idx, (movie,score) in enumerate(zip(top_hits.index, top_hits)):\n",
    "        print(\"%d '%s' with a similarity score of %.3f\" %(idx+1, movie, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our top 10 most similar movies for the keywords ['trump', 'china', 'virus', 'corona', 'coronavirus'] are:\n",
      "1 'Get ready for the corona coup_ectator.us/c' with a similarity score of 0.119\n",
      "2 'President Trump Projects Calm Amid Growing Coronavirus Fears, Appoints Vice President Pence to Lead US Response_w1.cbn.com/c' with a similarity score of 0.106\n",
      "3 'Indian migrant workers sprayed with disinfectant chemical_w.aljazeera.' with a similarity score of 0.104\n",
      "4 '‘You’re Not Funny’: Coronavirus-Stricken Chris Cuomo, Don Lemon Banter As One Anchor Tears Up_ilycaller.co' with a similarity score of 0.099\n",
      "5 'Kevin McCarthy: Pelosi Delayed Coronavirus Funding Bill So Dems Could Run Anti-GOP Ads_wnhall.com/t' with a similarity score of 0.098\n",
      "6 'Feds Tell Ex-NFL Star to Stop Saying His Weed Can Cure Coronavirus_w.thedailybe' with a similarity score of 0.089\n",
      "7 'Trump is throwing Georgia under the bus_w.theweek.co' with a similarity score of 0.084\n",
      "8 'Florida residents hit the beach while Texans protest coronavirus restrictions_post.com/202' with a similarity score of 0.084\n",
      "9 'What America needs to do before lockdown can end_w.theweek.co' with a similarity score of 0.081\n",
      "10 'Trump and the Coronavirus Death Rate_w.factcheck.' with a similarity score of 0.080\n"
     ]
    }
   ],
   "source": [
    "keywords_recommendation(['trump','china', 'virus', 'corona', 'coronavirus'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Model: Word counts with cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "path_df = \"Pickles/News_dataset.pickle\"\n",
    "\n",
    "with open(path_df, 'rb') as data:\n",
    "    df = pickle.load(data)\n",
    "    \n",
    "df['bag_of_words'] = ''\n",
    "columns = df.columns\n",
    "for index, row in df.iterrows():\n",
    "    words = ''\n",
    "    for col in columns:\n",
    "        if col == 'Content':\n",
    "            \n",
    "            words = words + row[col]+ ' '\n",
    "    row['bag_of_words'] = words\n",
    "text = df.bag_of_words.tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(text)\n",
    "vectors = vectorizer.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_recommender(title, number_of_hits=5):\n",
    "    df_index = df[df.Title == title].index.values[0]\n",
    "\n",
    "    cosines = []\n",
    "    for i in range(len(vectors)):\n",
    "        vector_list = [vectors[df_index], vectors[i]]\n",
    "        cosines.append(cosine_similarity(vector_list)[0,1])\n",
    "\n",
    "    cosines = pd.Series(cosines)\n",
    "    index = cosines.nlargest(number_of_hits+1).index\n",
    "\n",
    "    matches = df.loc[index]\n",
    "    for match,score in zip(matches['Title'][1:],cosines[index][1:]):\n",
    "        print(match,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China Tariffs_w.theflipsid 0.9261528960847949\n",
      "How Trump Can Make Trade with China Work for America_.nationalrev 0.9190024536822973\n",
      "White House Tries to Tamp Down Trade War Fears as China Retaliates_w.nytimes.co 0.9174586221858071\n",
      "U.S. to Put Tariffs on Chinese Goods, Drawing Vow of Retaliation as Trade Fight Widens_w.nytimes.co 0.9104299750679237\n",
      "The Trump Administration Debates a Cold War With China_w.theatlanti 0.9009608343087351\n"
     ]
    }
   ],
   "source": [
    "cosine_recommender(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Model: Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_sim(str1, str2):\n",
    "    a = set(str1.split(','))\n",
    "    b = set(str2.split(','))\n",
    "    c = a.intersection(b)\n",
    "    return(float(len(c)) / (len(a) + len(b) - len(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_recommender(title, number_of_hits=10):\n",
    "    df = df[df.Title==title]\n",
    "    keyword_string = movie.bag_of_words.iloc[0]\n",
    "\n",
    "    jaccards = []\n",
    "    for news in df['bag_of_words']:\n",
    "        jaccards.append(get_jaccard_sim(keyword_string, news))\n",
    "    jaccards = pd.Series(jaccards)\n",
    "    jaccards_index = jaccards.nlargest(number_of_hits+1).index\n",
    "    matches = df.loc[jaccards_index]\n",
    "    for match,score in zip(matches['Title'][1:],jaccards[jaccards_index][1:]) :\n",
    "        print(match,score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China Tariffs_w.theflipsid 0.9261528960847949\n",
      "How Trump Can Make Trade with China Work for America_.nationalrev 0.9190024536822973\n",
      "White House Tries to Tamp Down Trade War Fears as China Retaliates_w.nytimes.co 0.9174586221858071\n",
      "U.S. to Put Tariffs on Chinese Goods, Drawing Vow of Retaliation as Trade Fight Widens_w.nytimes.co 0.9104299750679237\n",
      "The Trump Administration Debates a Cold War With China_w.theatlanti 0.9009608343087351\n"
     ]
    }
   ],
   "source": [
    "cosine_recommender(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
