{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "0tVPqXjsCfkM",
    "outputId": "55224443-bcc6-43e7-eef8-7d3499aa709f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n",
      "Requirement already satisfied: gym[all] in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.18.4)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.4.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.5.0)\n",
      "Requirement already satisfied: Pillow; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (7.0.0)\n",
      "Requirement already satisfied: opencv-python; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (4.1.2.30)\n",
      "Collecting mujoco-py<2.0,>=1.50; extra == \"all\"\n",
      "  Using cached https://files.pythonhosted.org/packages/cf/8c/64e0630b3d450244feef0688d90eab2448631e40ba6bdbd90a70b84898e7/mujoco-py-1.50.1.68.tar.gz\n",
      "Requirement already satisfied: imageio; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (2.4.1)\n",
      "Requirement already satisfied: box2d-py~=2.3.5; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (2.3.8)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (0.2.6)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[all]) (0.16.0)\n",
      "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.11.2)\n",
      "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.29.19)\n",
      "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.14.0)\n",
      "Requirement already satisfied: lockfile>=0.12.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.12.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"all\"->gym[all]) (1.12.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (2.20)\n",
      "Building wheels for collected packages: mujoco-py\n",
      "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for mujoco-py\n",
      "Failed to build mujoco-py\n",
      "Installing collected packages: mujoco-py\n",
      "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-x2m0vugo/mujoco-py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-x2m0vugo/mujoco-py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-x8ei1mq3/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install 'gym[all]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "uo9IhQoeCqNv",
    "outputId": "ed3f3e7c-af01-4f6f-e319-7e7b0521f2d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "mydrive =\"/content/drive/My Drive/Colab Notebooks/DS_hw4_lunar_lander/\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80bcxPA9CqKl"
   },
   "outputs": [],
   "source": [
    "class AC(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent):\n",
    "        super(AC, self).__init__()\n",
    "        self.action_layer = nn.Sequential(nn.Linear(state_dim, n_latent), nn.Tanh(), # actor\n",
    "                                          nn.Linear(n_latent, n_latent), nn.Tanh(),\n",
    "                                          nn.Linear(n_latent, action_dim), nn.Softmax(dim = -1))\n",
    "        self.value_layer = nn.Sequential(nn.Linear(state_dim, n_latent), nn.Tanh(), # critic\n",
    "                                         nn.Linear(n_latent, n_latent), nn.Tanh(),\n",
    "                                         nn.Linear(n_latent, 1))  \n",
    "\n",
    "    def action(self, state, ex):\n",
    "        state = torch.from_numpy(state).double().to(device)\n",
    "        action_probability = self.action_layer(state)\n",
    "        distance = Categorical(action_probability)\n",
    "        action = distance.sample()\n",
    "\n",
    "        ex.states.append(state) # add conditions in experience replay\n",
    "        ex.actions.append(action) \n",
    "        ex.logprobability.append(distance.log_prob(action))\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def evaluation(self, state, action):\n",
    "        action_probability = self.action_layer(state)\n",
    "        distance = Categorical(action_probability)\n",
    "        action_logprobability = distance.log_prob(action)\n",
    "        d_entropy = distance.entropy()\n",
    "        state_value = self.value_layer(state)\n",
    "\n",
    "        return action_logprobability, torch.squeeze(state_value), d_entropy\n",
    "    \n",
    "\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bfj7sacBlWK"
   },
   "outputs": [],
   "source": [
    "class Experience:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_state = []\n",
    "        self.states = []\n",
    "        self.logprobability = []\n",
    "\n",
    "    def clear_out_ex(self):\n",
    "        del self.actions[:]\n",
    "        del self.rewards[:]\n",
    "        del self.terminal_state[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobability[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTh0rQ0F_unU"
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, n_latent, lr, betas, gamma, K_epochs, clip):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.clip = clip\n",
    "        self.betas = betas\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.policy = AC(state_dim, action_dim, n_latent).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.old_policy = AC(state_dim, action_dim, n_latent).to(device)\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def update(self, ex):\n",
    "        rewards = []\n",
    "        minus_reward = 0\n",
    "        for reward, terminal_state in zip(reversed(ex.rewards), reversed(ex.terminal_state)):\n",
    "            if terminal_state == True:\n",
    "                minus_reward = 0\n",
    "            minus_reward = reward + (self.gamma * minus_reward)\n",
    "            rewards.insert(0, minus_reward)\n",
    "\n",
    "        rewards = torch.tensor(rewards).to(device) # normalized the rewards\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 0.00001)\n",
    "\n",
    "        old_states = torch.stack(ex.states).to(device).detach() # convert list to tensor\n",
    "        old_actions = torch.stack(ex.actions).to(device).detach()\n",
    "        old_logprob = torch.stack(ex.logprobability).to(device).detach()\n",
    "\n",
    "        for i in range(self.K_epochs):\n",
    "            logprobs, state_val, d_entropy = self.policy.evaluation(old_states, old_actions)\n",
    "            ratios = torch.exp(logprobs-old_logprob.detach())\n",
    "\n",
    "            advantages = rewards - state_val.detach()\n",
    "            surrogate1 = ratios*advantages\n",
    "            surrogate2 = torch.clamp(ratios, 1-self.clip, 1+self.clip)*advantages\n",
    "            loss = -torch.min(surrogate1, surrogate2)+0.5*self.MseLoss(state_val, rewards)-0.01*d_entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LjWZ15vWFs86"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    env_name = \"LunarLander-v2\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    render = False\n",
    "    solve_reward = 150\n",
    "    log_interval = 20\n",
    "    max_epoch = 50000\n",
    "    max_timestep = 300\n",
    "    update_timestep = 2000\n",
    "\n",
    "    ex = Experience()\n",
    "    ppo = PPO(state_dim, 4, 64, 0.002, (0.9, 0.999), 0.99, 4, 0.2)\n",
    "\n",
    "    running_reward, avg_length, timestep = 0, 0, 0\n",
    "    \n",
    "\n",
    "    for i in range(1, max_epoch+1):\n",
    "        state = env.reset()\n",
    "        for t in range(max_timestep):\n",
    "            timestep += 1\n",
    "            action = ppo.old_policy.action(state, ex)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            ex.rewards.append(reward)\n",
    "            ex.terminal_state.append(done)\n",
    "\n",
    "            if timestep % update_timestep == 0:\n",
    "                ppo.update(ex)\n",
    "                ex.clear_out_ex()\n",
    "                timestep = 0\n",
    "\n",
    "            running_reward += reward\n",
    "            if render: \n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "        avg_length += t\n",
    "\n",
    "        if running_reward > (log_interval*solve_reward):\n",
    "            print(\"solveeeee\")\n",
    "            torch.save(ppo.policy.state_dict(), mydrive+'PPO_v2.pth')\n",
    "            break\n",
    "\n",
    "        if i % log_interval == 0:\n",
    "            avg_length = int(avg_length/log_interval)\n",
    "            running_reward = int((running_reward/log_interval))\n",
    "            print('Episode {} \\t avg length: {} \\t reward: {}'.format(i, avg_length, running_reward))\n",
    "            running_reward = 0\n",
    "            avg_length = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "uJrVJFhzCqI-",
    "outputId": "cfd72b7a-e8de-4e4e-aca7-b72f48c6833a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
      "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
      "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f2bf2bfd6a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!apt-get install xvfb\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install Pillow\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujHtFrW1CqGH"
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJyX0bbgCqC1"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import gym\n",
    "\n",
    "def test():\n",
    "    env_name = \"LunarLander-v2\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    \n",
    "    max_timestep = 500\n",
    "    render = True\n",
    "    save_gif = False\n",
    "    \n",
    "    ex = Experience()\n",
    "    ppo = PPO(state_dim, 4, 64, 0.0007, (0.9, 0.999), 0.99, 4, 0.2)\n",
    "    ppo.old_policy.load_state_dict(torch.load(mydrive+\"PPO_v2.pth\"))\n",
    "    \n",
    "    for ep in range(1, 4):\n",
    "        ep_reward = 0\n",
    "        state = env.reset()\n",
    "        for t in range(max_timestep):\n",
    "            action = ppo.old_policy.action(state, ex)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if save_gif:\n",
    "                 img = env.render(mode = 'rgb_array')\n",
    "                 img = Image.fromarray(img)\n",
    "                 img.save(mydrive+'gif_{}.jpg'.format(t))  \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        print('Episode: {}\\tReward: {}'.format(ep, int(ep_reward)))\n",
    "        ep_reward = 0\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "kxeuCBTRCp_p",
    "outputId": "18cefb69-3799-452a-aca7-5d1cc7797a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\tReward: 166\n",
      "Episode: 2\tReward: 124\n",
      "Episode: 3\tReward: 176\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgXP2HHCCp9N"
   },
   "outputs": [],
   "source": [
    "import os, signal\n",
    "os.kill(os.getpid(), signal.SIGKILL)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "my_version.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
