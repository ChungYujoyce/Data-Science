{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "pytorch_v.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqptKy98d6tk",
        "colab_type": "text"
      },
      "source": [
        "### Actor-Critic version Lunar Lander"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9nfdw80gtMq",
        "colab_type": "code",
        "outputId": "043b8e06-c2ed-4e54-b1c5-a6d3d5dd38a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "!pip install torch\n",
        "!pip install 'gym[all]'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.18.4)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.3.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (7.0.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (2.3.8)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (0.2.6)\n",
            "Requirement already satisfied: imageio; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (2.4.1)\n",
            "Collecting mujoco-py<2.0,>=1.50; extra == \"all\"\n",
            "  Using cached https://files.pythonhosted.org/packages/cf/8c/64e0630b3d450244feef0688d90eab2448631e40ba6bdbd90a70b84898e7/mujoco-py-1.50.1.68.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[all]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"all\"->gym[all]) (1.12.0)\n",
            "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.11.1)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.29.18)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.14.0)\n",
            "Requirement already satisfied: lockfile>=0.12.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (2.20)\n",
            "Building wheels for collected packages: mujoco-py\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-f37kpk8y/mujoco-py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-f37kpk8y/mujoco-py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-djuyd3t8/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5_iQBfagmD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import gym\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.affine = nn.Linear(8, 128)\n",
        "        \n",
        "        self.action_layer = nn.Linear(128, 4)\n",
        "        self.value_layer = nn.Linear(128, 1)\n",
        "        \n",
        "        self.logprobs = []\n",
        "        self.state_values = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = torch.from_numpy(state).double()\n",
        "        state = F.relu(self.affine(state))\n",
        "        \n",
        "        state_value = self.value_layer(state)\n",
        "        \n",
        "        action_probs = F.softmax(self.action_layer(state))\n",
        "        action_distribution = Categorical(action_probs)\n",
        "        action = action_distribution.sample()\n",
        "        \n",
        "        self.logprobs.append(action_distribution.log_prob(action))\n",
        "        self.state_values.append(state_value)\n",
        "        \n",
        "        return action.item()\n",
        "    \n",
        "    def calculateLoss(self, gamma=0.99):\n",
        "        \n",
        "        # calculating discounted rewards:\n",
        "        rewards = []\n",
        "        dis_reward = 0\n",
        "        for reward in self.rewards[::-1]:\n",
        "            dis_reward = reward + gamma * dis_reward\n",
        "            rewards.insert(0, dis_reward)\n",
        "                \n",
        "        # normalizing the rewards:\n",
        "        rewards = torch.tensor(rewards)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std())\n",
        "        \n",
        "        loss = 0\n",
        "        for logprob, value, reward in zip(self.logprobs, self.state_values, rewards):\n",
        "            advantage = reward  - value.item()\n",
        "            action_loss = -logprob * advantage\n",
        "            value_loss = F.smooth_l1_loss(value, reward)\n",
        "            loss += (action_loss + value_loss)   \n",
        "        return loss\n",
        "    \n",
        "    def clearMemory(self):\n",
        "        del self.logprobs[:]\n",
        "        del self.state_values[:]\n",
        "        del self.rewards[:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKv7gaCYh5-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "def test(n_episodes=5, name='LunarLander_v1.pth'):\n",
        "    env = gym.make('LunarLander-v2')\n",
        "    policy = ActorCritic()\n",
        "    \n",
        "    policy.load_state_dict(torch.load(mydrive+'{}'.format(name)))\n",
        "    \n",
        "    render = True\n",
        "    save_gif = True\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        running_reward = 0\n",
        "        for t in range(10000):\n",
        "            action = policy(state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            running_reward += reward\n",
        "            if render:\n",
        "                 env.render()\n",
        "                 if save_gif:\n",
        "                     img = env.render(mode = 'rgb_array')\n",
        "                     img = Image.fromarray(img)\n",
        "                     img.save(mydrive+'gif_{}.jpg'.format(t))\n",
        "            if done:\n",
        "                break\n",
        "        print('Episode {}\\tReward: {}'.format(i_episode, running_reward))\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0O3kgEPg9BX",
        "colab_type": "code",
        "outputId": "2ee9305d-d9ec-41e6-ea9c-858e5673b5a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mydrive =\"/content/drive/My Drive/Colab Notebooks/DS_hw4_lunar_lander/\"\n",
        "\n",
        "def train():\n",
        "\n",
        "    render = False\n",
        "    gamma = 0.99\n",
        "    lr = 0.02\n",
        "    betas = (0.9, 0.999)\n",
        "    random_seed = 543\n",
        "    \n",
        "    torch.manual_seed(random_seed)\n",
        "    \n",
        "    env = gym.make('LunarLander-v2')\n",
        "    env.seed(random_seed)\n",
        "    \n",
        "    policy = ActorCritic()\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=lr, betas=betas)\n",
        "    print(lr,betas)\n",
        "    \n",
        "    running_reward = 0\n",
        "    for i_episode in range(0, 10000):\n",
        "        state = env.reset()\n",
        "        for t in range(10000):\n",
        "            action = policy(state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            policy.rewards.append(reward)\n",
        "            running_reward += reward\n",
        "            if render and i_episode > 1000:\n",
        "                env.render()\n",
        "            if done:\n",
        "                break\n",
        "                    \n",
        "        # Updating the policy :\n",
        "        optimizer.zero_grad()\n",
        "        loss = policy.calculateLoss(gamma)\n",
        "        loss.backward()\n",
        "        optimizer.step()        \n",
        "        policy.clearMemory()\n",
        "        \n",
        "        # saving the model if episodes > 999 OR avg reward > 200 \n",
        "        #if i_episode > 999:\n",
        "        #    torch.save(policy.state_dict(), './preTrained/LunarLander_{}_{}_{}.pth'.format(lr, betas[0], betas[1]))\n",
        "        \n",
        "        if running_reward > 4000:\n",
        "            #torch.save(policy.state_dict(), mydrive+'LunarLander_v1.pth'.format(lr, betas[0], betas[1]))\n",
        "            print(\"########## Solved! ##########\")\n",
        "            test(name='LunarLander_v1.pth'.format(lr, betas[0], betas[1]))\n",
        "            break\n",
        "        \n",
        "        if i_episode % 20 == 0:\n",
        "            running_reward = running_reward/20\n",
        "            print('Episode {}\\tlength: {}\\treward: {}'.format(i_episode, t, running_reward))\n",
        "            running_reward = 0\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkaubuqjdGIn",
        "colab_type": "text"
      },
      "source": [
        "### Below is for Colab to have the gym display window"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNTpN00p6bCG",
        "colab_type": "code",
        "outputId": "d88204f4-b678-49b5-cb35-3c8b924591a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!apt-get install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install Pillow\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f870ea9cac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN83iPludjkv",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rbZqWUUg8_C",
        "colab_type": "code",
        "outputId": "ba73df49-0f89-401c-b248-e6c0b1e933b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.set_default_tensor_type(torch.DoubleTensor)\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.02 (0.9, 0.999)\n",
            "Episode 0\tlength: 81\treward: -6.557279878072909\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 20\tlength: 154\treward: -396.04188633687227\n",
            "Episode 40\tlength: 118\treward: -267.82300554392606\n",
            "Episode 60\tlength: 150\treward: -517.5924279392121\n",
            "Episode 80\tlength: 153\treward: -423.0069357130834\n",
            "Episode 100\tlength: 113\treward: -326.1209005643042\n",
            "Episode 120\tlength: 98\treward: -242.49694728043406\n",
            "Episode 140\tlength: 89\treward: -449.4194637776465\n",
            "Episode 160\tlength: 87\treward: -482.7429554841181\n",
            "Episode 180\tlength: 69\treward: -202.55485482418368\n",
            "Episode 200\tlength: 81\treward: -72.84484324908954\n",
            "Episode 220\tlength: 153\treward: -67.53457492940767\n",
            "Episode 240\tlength: 98\treward: -55.19903008630845\n",
            "Episode 260\tlength: 142\treward: -71.64755129128199\n",
            "Episode 280\tlength: 114\treward: -154.9368873101258\n",
            "Episode 300\tlength: 164\treward: -94.65030034212754\n",
            "Episode 320\tlength: 114\treward: -65.86661435346787\n",
            "Episode 340\tlength: 240\treward: -142.0397544048803\n",
            "Episode 360\tlength: 113\treward: -7.190940598384961\n",
            "Episode 380\tlength: 90\treward: -13.850279174129543\n",
            "Episode 400\tlength: 999\treward: -31.896496560252377\n",
            "Episode 420\tlength: 533\treward: -167.72595909472014\n",
            "Episode 440\tlength: 107\treward: 15.139900602529238\n",
            "Episode 460\tlength: 109\treward: -2.381383773087257\n",
            "Episode 480\tlength: 999\treward: 25.760769990764015\n",
            "Episode 500\tlength: 98\treward: 24.396068747841376\n",
            "Episode 520\tlength: 84\treward: -8.112633434886545\n",
            "Episode 540\tlength: 105\treward: -10.359116183660918\n",
            "Episode 560\tlength: 352\treward: 15.981948831060397\n",
            "Episode 580\tlength: 133\treward: -26.934258157859848\n",
            "Episode 600\tlength: 87\treward: -30.692083237474\n",
            "Episode 620\tlength: 132\treward: 42.808327701430144\n",
            "Episode 640\tlength: 265\treward: 55.447502891734544\n",
            "Episode 660\tlength: 661\treward: 58.21749017306778\n",
            "Episode 680\tlength: 211\treward: 70.35205757349966\n",
            "Episode 700\tlength: 119\treward: 0.3060176177746371\n",
            "Episode 720\tlength: 124\treward: 6.983869297775692\n",
            "Episode 740\tlength: 125\treward: 24.21127854731189\n",
            "Episode 760\tlength: 186\treward: 159.17501666524296\n",
            "Episode 780\tlength: 480\treward: 77.06971429338077\n",
            "Episode 800\tlength: 194\treward: -60.57491026973844\n",
            "Episode 820\tlength: 310\treward: 139.91903861701533\n",
            "Episode 840\tlength: 155\treward: 93.73931904323548\n",
            "Episode 860\tlength: 185\treward: 164.68134109540782\n",
            "Episode 880\tlength: 253\treward: 135.43161183693707\n",
            "Episode 900\tlength: 397\treward: 130.396809300553\n",
            "Episode 920\tlength: 159\treward: 98.11736578930024\n",
            "Episode 940\tlength: 135\treward: 118.98843261917996\n",
            "Episode 960\tlength: 175\treward: 198.11829463544026\n",
            "Episode 980\tlength: 136\treward: 130.3790677522589\n",
            "Episode 1000\tlength: 372\treward: 155.0528370037614\n",
            "Episode 1020\tlength: 202\treward: 193.112741614541\n",
            "Episode 1040\tlength: 177\treward: 125.84665034747836\n",
            "Episode 1060\tlength: 205\treward: 181.51091753426118\n",
            "########## Solved! ##########\n",
            "Episode 1\tReward: -16.110357143806553\n",
            "Episode 2\tReward: 266.5470501288912\n",
            "Episode 3\tReward: -1.737872522915481\n",
            "Episode 4\tReward: 281.16218854262644\n",
            "Episode 5\tReward: 274.3783790280032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hhsr1vqdX5g",
        "colab_type": "text"
      },
      "source": [
        "### Clear out all "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzhwp_rzjXDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7cjKAPgjXiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}